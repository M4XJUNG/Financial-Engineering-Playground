{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aab9d32-5769-4ee7-a5d3-57c561dfb242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.53.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.33.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.33.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.24.1)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.7.0 scipy-1.15.3 threadpoolctl-3.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b778967-0254-4cbd-a59b-0364bf52c233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.53.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "651a8b9b-7ea0-4f6d-84c0-15e8cb019ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드를 위함\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 기본 파이썬 패키지\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# GPT 사용을 위함\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# 전처리 및 평가 지표\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c2ab178-ed86-4625-912c-9bb0720ce1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-05 07:31:53--  https://raw.githubusercontent.com/ukairia777/finance_sentiment_corpus/main/finance_data.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1319001 (1.3M) [text/plain]\n",
      "Saving to: ‘finance_data.csv.1’\n",
      "\n",
      "finance_data.csv.1  100%[===================>]   1.26M  --.-KB/s    in 0.05s   \n",
      "\n",
      "2025-07-05 07:31:53 (27.7 MB/s) - ‘finance_data.csv.1’ saved [1319001/1319001]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/ukairia777/finance_sentiment_corpus/main/finance_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "561cf85c-0e98-426f-b87f-2c3481dd5759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플의 개수 : 4846\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('finance_data.csv')\n",
    "print('샘플의 개수 :', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "588ba5a7-c84e-4afe-bbfc-abb492409641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>sentence</th>\n",
       "      <th>kor_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran, the company has no plans to...</td>\n",
       "      <td>Gran에 따르면, 그 회사는 회사가 성장하고 있는 곳이지만, 모든 생산을 러시아로...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "      <td>테크노폴리스는 컴퓨터 기술과 통신 분야에서 일하는 회사들을 유치하기 위해 10만 평...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "      <td>국제 전자산업 회사인 엘코텍은 탈린 공장에서 수십 명의 직원을 해고했으며, 이전의 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "      <td>새로운 생산공장으로 인해 회사는 예상되는 수요 증가를 충족시킬 수 있는 능력을 증가...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company's updated strategy fo...</td>\n",
       "      <td>2009-2012년 회사의 업데이트된 전략에 따르면, Basware는 20% - 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     labels                                           sentence  \\\n",
       "0   neutral  According to Gran, the company has no plans to...   \n",
       "1   neutral  Technopolis plans to develop in stages an area...   \n",
       "2  negative  The international electronic industry company ...   \n",
       "3  positive  With the new production plant the company woul...   \n",
       "4  positive  According to the company's updated strategy fo...   \n",
       "\n",
       "                                        kor_sentence  \n",
       "0  Gran에 따르면, 그 회사는 회사가 성장하고 있는 곳이지만, 모든 생산을 러시아로...  \n",
       "1  테크노폴리스는 컴퓨터 기술과 통신 분야에서 일하는 회사들을 유치하기 위해 10만 평...  \n",
       "2  국제 전자산업 회사인 엘코텍은 탈린 공장에서 수십 명의 직원을 해고했으며, 이전의 ...  \n",
       "3  새로운 생산공장으로 인해 회사는 예상되는 수요 증가를 충족시킬 수 있는 능력을 증가...  \n",
       "4  2009-2012년 회사의 업데이트된 전략에 따르면, Basware는 20% - 4...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba473fea-dcfa-4f11-aa6b-6485e8fe829c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "neutral     2879\n",
       "positive    1363\n",
       "negative     604\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2d0f522-f6ee-40b5-a6d1-959bf4e05f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1329/2740635050.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['labels'] = df['labels'].replace(['neutral', 'positive', 'negative'],[0, 1, 2])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>sentence</th>\n",
       "      <th>kor_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>According to Gran, the company has no plans to...</td>\n",
       "      <td>Gran에 따르면, 그 회사는 회사가 성장하고 있는 곳이지만, 모든 생산을 러시아로...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "      <td>테크노폴리스는 컴퓨터 기술과 통신 분야에서 일하는 회사들을 유치하기 위해 10만 평...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "      <td>국제 전자산업 회사인 엘코텍은 탈린 공장에서 수십 명의 직원을 해고했으며, 이전의 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "      <td>새로운 생산공장으로 인해 회사는 예상되는 수요 증가를 충족시킬 수 있는 능력을 증가...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>According to the company's updated strategy fo...</td>\n",
       "      <td>2009-2012년 회사의 업데이트된 전략에 따르면, Basware는 20% - 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels                                           sentence  \\\n",
       "0       0  According to Gran, the company has no plans to...   \n",
       "1       0  Technopolis plans to develop in stages an area...   \n",
       "2       2  The international electronic industry company ...   \n",
       "3       1  With the new production plant the company woul...   \n",
       "4       1  According to the company's updated strategy fo...   \n",
       "\n",
       "                                        kor_sentence  \n",
       "0  Gran에 따르면, 그 회사는 회사가 성장하고 있는 곳이지만, 모든 생산을 러시아로...  \n",
       "1  테크노폴리스는 컴퓨터 기술과 통신 분야에서 일하는 회사들을 유치하기 위해 10만 평...  \n",
       "2  국제 전자산업 회사인 엘코텍은 탈린 공장에서 수십 명의 직원을 해고했으며, 이전의 ...  \n",
       "3  새로운 생산공장으로 인해 회사는 예상되는 수요 증가를 충족시킬 수 있는 능력을 증가...  \n",
       "4  2009-2012년 회사의 업데이트된 전략에 따르면, Basware는 20% - 4...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['labels'] = df['labels'].replace(['neutral', 'positive', 'negative'],[0, 1, 2])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "572313ae-d5fa-4125-914d-7f69261dd983",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('finance_data.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6e65c32-9238-4c4c-aa5a-45e214090703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009e4fd8499444f2b6252a2c95b294dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_data = load_dataset(\n",
    "        \"csv\",\n",
    "        data_files={\n",
    "            \"train\": \"finance_data.csv\",\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "426b9565-e7cf-48bb-b7ec-bce116659e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = all_data['train'].train_test_split(0.2, seed=777)\n",
    "train_cs = cs[\"train\"]\n",
    "test_cs = cs[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e424470c-e1ab-4067-884a-0d390a5f540c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'sentence', 'kor_sentence'],\n",
       "    num_rows: 3876\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cbf5265-701e-4f2b-84b5-aa2db8b3900b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'sentence', 'kor_sentence'],\n",
       "    num_rows: 970\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74c6a53a-bda1-4572-8508-a6499f0b5ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터를 다시 8:2로 분리 후 훈련 데이터와 검증 데이터로 저장\n",
    "cs = train_cs.train_test_split(0.2, seed=777)\n",
    "train_cs = cs[\"train\"]\n",
    "valid_cs = cs[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44193b4c-2e31-4f4d-b0ef-667f1ed709d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'sentence', 'kor_sentence'],\n",
       "    num_rows: 3100\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 데이터\n",
    "train_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18f5b2de-7ec3-465f-b59b-35ff875641ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'sentence', 'kor_sentence'],\n",
       "    num_rows: 776\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 검증 데이터\n",
    "valid_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75e15059-3acb-4345-81bc-409b6a4f3b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'sentence', 'kor_sentence'],\n",
       "    num_rows: 970\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트 데이터\n",
    "test_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f550cecd-050a-4b04-b61c-21c253491bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "두번째 샘플 출력 : 알마 미디어 코퍼레이션 사업 ID 1944757-4의 주식 자본금은 44,767,513.80유로이며 74,612,523주로 나뉜다.\n",
      "두번째 샘플의 레이블 출력 : 0\n"
     ]
    }
   ],
   "source": [
    "print('두번째 샘플 출력 :', train_cs['kor_sentence'][1])\n",
    "print('두번째 샘플의 레이블 출력 :', train_cs['labels'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b9c69ad-1fab-455f-9ba5-8bcb206f6edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터, 검증 데이터, 테스트 데이터\n",
    "train_sentences = list(train_cs['kor_sentence'])\n",
    "validation_sentences = list(valid_cs['kor_sentence'])\n",
    "test_sentences = list(test_cs['kor_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b06cc31-5928-4c62-9d8b-88e18aacbd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_cs['labels']\n",
    "validation_labels = valid_cs['labels']\n",
    "test_labels = test_cs['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d4711c4-54f9-40cb-90c8-923d361a58df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오전 10.58시 아우토쿰푸는 2.74pct 하락한 24.87유로, OMX 헬싱키 25지수는 0.55pct 상승한 2,825.14, OMX 헬싱키는 0.64pct 하락한 9,386.89유로에 거래됐다.',\n",
       " '10월부터 12월까지의 판매량은 302 mln 유로로 전년 동기 대비 25.3 pct 증가했다.',\n",
       " '매디슨, 위스콘신, 2월 6일 - PRNewswire - - 피스카스는 미국 특허청이 상징적인 가위 손잡이에 오렌지색 상표 등록을 허가했다고 발표한다.',\n",
       " \"M-real로 평가된 분석가들 중 총 6명은 ''매수' - ''누적''을 주었고, 3명은 ''보유'', 1명만이 ''매도''를 주었다.\",\n",
       " '주요 양조업체들은 지난해 국내 맥주 판매량을 2004년 2억4592만 리터에서 2억5688만 리터로 4.5% 늘렸다.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dbeee1db-c4df-4125-9d9b-12c019e64588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5597967f-5c08-4453-819a-f15bee5a5302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 GPT 중 하나인 'skt/kogpt2-base-v2'를 사용.\n",
    "tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4bb46bce-a5a4-47b5-b7b9-2a70c228a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', \n",
    "                  truncating='pre', value=0.0):\n",
    "    \"\"\"\n",
    "    시퀀스들을 동일한 길이로 패딩하는 함수\n",
    "    \n",
    "    Args:\n",
    "        sequences: 패딩할 시퀀스들의 리스트\n",
    "        maxlen: 최대 시퀀스 길이. None이면 가장 긴 시퀀스 길이 사용\n",
    "        dtype: 출력 배열의 데이터 타입\n",
    "        padding: 'pre' (앞쪽 패딩) 또는 'post' (뒤쪽 패딩)\n",
    "        truncating: 'pre' (앞쪽 자르기) 또는 'post' (뒤쪽 자르기)\n",
    "        value: 패딩에 사용할 값\n",
    "    \n",
    "    Returns:\n",
    "        패딩된 numpy 배열\n",
    "    \"\"\"\n",
    "    if not sequences:\n",
    "        return np.array([])\n",
    "    \n",
    "    # 최대 길이 결정\n",
    "    if maxlen is None:\n",
    "        maxlen = max(len(seq) for seq in sequences)\n",
    "    \n",
    "    # 결과 배열 초기화\n",
    "    if dtype == 'long':\n",
    "        result = np.full((len(sequences), maxlen), value, dtype=np.int64)\n",
    "    else:\n",
    "        result = np.full((len(sequences), maxlen), value, dtype=dtype)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        if len(seq) == 0:\n",
    "            continue\n",
    "        \n",
    "        # 시퀀스가 maxlen보다 긴 경우 자르기\n",
    "        if len(seq) > maxlen:\n",
    "            if truncating == 'pre':\n",
    "                seq = seq[-maxlen:]  # 앞쪽 자르기\n",
    "            else:  # truncating == 'post'\n",
    "                seq = seq[:maxlen]   # 뒤쪽 자르기\n",
    "        \n",
    "        # 패딩 적용\n",
    "        if padding == 'pre':\n",
    "            # 앞쪽 패딩: 뒤쪽부터 채우기\n",
    "            result[i, -len(seq):] = seq\n",
    "        else:  # padding == 'post'\n",
    "            # 뒤쪽 패딩: 앞쪽부터 채우기\n",
    "            result[i, :len(seq)] = seq\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90b80ec6-81ff-466f-b99a-e7034c90b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이는 128\n",
    "max_len = 128\n",
    "\n",
    "def data_to_tensor (sentences, labels, MAX_LEN):\n",
    "  # 정수 인코딩 과정. 각 텍스트를 토큰화한 후에 Vocabulary에 맵핑되는 정수 시퀀스로 변환한다.\n",
    "  # ex) ['안녕하세요'] ==> ['안', '녕', '하세요'] ==> [231, 52, 45]\n",
    "  tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "  input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "  # pad_sequences는 패딩을 위한 모듈. 주어진 최대 길이를 위해서 뒤에서 패딩 토큰의 번호로 채워준다.\n",
    "  # ex) [231, 52, 45] ==> [231, 52, 45, 패딩 토큰, 패딩 토큰, 패딩 토큰]\n",
    "  pad_token = tokenizer.encode('<pad>')[0]\n",
    "  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, value=pad_token, dtype=\"long\", truncating=\"post\", padding=\"post\") \n",
    "\n",
    "  attention_masks = []\n",
    "\n",
    "  for seq in input_ids:\n",
    "      seq_mask = [float(i != pad_token) for i in seq]\n",
    "      attention_masks.append(seq_mask)\n",
    "\n",
    "  tensor_inputs = torch.tensor(input_ids)\n",
    "  tensor_labels = torch.tensor(labels)\n",
    "  tensor_masks = torch.tensor(attention_masks)\n",
    "\n",
    "  return tensor_inputs, tensor_labels, tensor_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8761dbef-0fd8-4406-8fb6-dcb3768d60f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터, 검증 데이터, 테스트 데이터에 대해서\n",
    "# 정수 인코딩 결과, 레이블, 어텐션 마스크를 각각 inputs, labels, masks에 저장.\n",
    "train_inputs, train_labels, train_masks = data_to_tensor(train_sentences, train_labels, max_len)\n",
    "validation_inputs, validation_labels, validation_masks = data_to_tensor(validation_sentences, validation_labels, max_len)\n",
    "test_inputs, test_labels, test_masks = data_to_tensor(test_sentences, test_labels, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c68c2187-306c-409c-bb12-52a38206d0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 결과: tensor([26098,  9292, 10035,   399,  7888, 12844,  8563,  8488, 40174, 10621,\n",
      "        17405,   454, 19889, 24868,  8704, 10656, 11452,   398,  8125, 10034,\n",
      "        11324,   419,   430, 50061, 10373,  8263, 10583, 20853,   396,   454,\n",
      "        19889, 12734,  8704, 38695, 10355, 10179, 16366, 11324,   419,   430,\n",
      "        42880, 11032, 32419,   395,   454, 19889, 24868,  8704, 17629, 14915,\n",
      "          397, 11452,   400,  8125, 17329, 10664,  7250,  9016,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3])\n",
      "--------------------\n",
      "원본 문장 복원 결과: 오전 10.58시 아우토쿰푸는 2.74pct 하락한 24.87유로, OMX 헬싱키 25지수는 0.55pct 상승한 2,825.14, OMX 헬싱키는 0.64pct 하락한 9,386.89유로에 거래됐다.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "--------------------\n",
      "어텐션 마스크: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n",
      "--------------------\n",
      "샘플의 길이: 128\n",
      "--------------------\n",
      "레이블: tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# 첫 번째 샘플의 정수 인코딩 결과\n",
    "sample_input = test_inputs[0]\n",
    "sample_label = test_labels[0]\n",
    "sample_mask = test_masks[0]\n",
    "\n",
    "print(\"정수 인코딩 결과:\", sample_input)\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# 원본 문장 복원\n",
    "decoded_text = tokenizer.decode(sample_input)\n",
    "print(\"원본 문장 복원 결과:\", decoded_text)\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# 어텐션 마스크\n",
    "print(\"어텐션 마스크:\", sample_mask)\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# 샘플의 길이\n",
    "print(\"샘플의 길이:\", len(sample_input))\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# 레이블\n",
    "print(\"레이블:\", sample_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "845f1430-a795-41a9-94a8-3402d608a06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "29ab75e5-abcb-45f8-87c6-11be1761add0",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c35d671c-6864-4a85-bc96-7d292b6c4d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d7980b5-169f-4b2a-a4ca-ad01a41742dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9ccc021e-a942-457f-b1cc-6293e5a9432d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at skt/kogpt2-base-v2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(51200, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=3, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = 3\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"skt/kogpt2-base-v2\", num_labels=num_labels)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9255bbdb-ab60-4921-81e9-c593942cc748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 몇 번의 에포크(전체 데이터에 대한 학습 횟수)를 할 것인지 선택\n",
    "epochs = 3\n",
    "\n",
    "# 옵티마이저 선택\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "86396d7f-059a-427f-a861-ee63a3a9df0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(predictions, labels):\n",
    "    # predictions: 모델이 예측한 결과값들의 리스트 또는 배열\n",
    "    # labels: 실제 정답 레이블들의 리스트 또는 배열\n",
    "\n",
    "    # 예측값과 실제 레이블을 별도의 변수에 할당\n",
    "    y_pred = predictions\n",
    "    y_true = labels\n",
    "\n",
    "    # 사용 가능한 메트릭들을 계산\n",
    "\n",
    "    # 정확도 (Accuracy)\n",
    "    # 전체 예측 중에서 올바르게 예측한 비율\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # 매크로 평균 F1 점수 (Macro-averaged F1 Score)\n",
    "    # 클래스별로 F1 점수를 계산한 후, 그 평균을 구함\n",
    "    # zero_division=0 옵션은 분모가 0일 경우 0을 반환하도록 설정\n",
    "    f1_macro_average = f1_score(y_true=y_true, y_pred=y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    # 마이크로 평균 F1 점수 (Micro-averaged F1 Score)\n",
    "    # 전체 데이터에 대해 단일 F1 점수를 계산\n",
    "    # 클래스 불균형이 심한 경우에 적합\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro', zero_division=0)\n",
    "\n",
    "    # 가중 평균 F1 점수 (Weighted-averaged F1 Score)\n",
    "    # 각 클래스의 F1 점수에 해당 클래스의 샘플 수를 가중치로 곱한 후 평균을 구함\n",
    "    f1_weighted_average = f1_score(y_true=y_true, y_pred=y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    # 계산된 메트릭 결과를 딕셔너리 형태로 리턴\n",
    "    metrics = {'accuracy': accuracy,\n",
    "               'f1_macro': f1_macro_average,\n",
    "               'f1_micro': f1_micro_average,\n",
    "               'f1_weighted': f1_weighted_average}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76017f6c-6320-4012-8edf-e8d4f0983a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_dataloader, optimizer, device):\n",
    "    \"\"\"\n",
    "    하나의 에포크 동안 모델을 학습시키는 함수입니다.\n",
    "\n",
    "    Parameters:\n",
    "    model (torch.nn.Module): 학습시킬 모델 객체.\n",
    "    train_dataloader (torch.utils.data.DataLoader): 학습 데이터셋의 DataLoader.\n",
    "    optimizer (torch.optim.Optimizer): 최적화 알고리즘을 구현하는 객체.\n",
    "    device (torch.device): 학습에 사용할 장치(CPU 또는 CUDA).\n",
    "\n",
    "    Returns:\n",
    "    float: 평균 학습 손실값.\n",
    "    \"\"\"\n",
    "\n",
    "    total_train_loss = 0  # 학습 손실을 누적할 변수 초기화\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "\n",
    "    # 학습 데이터로더를 순회하며 배치 단위로 학습\n",
    "    for step, batch in tqdm(enumerate(train_dataloader), desc=\"Training Batch\"):\n",
    "        batch = tuple(t.to(device) for t in batch)  # DataLoader에서 배치를 받아 각 텐서를 지정된 장치로 이동\n",
    "        b_input_ids, b_input_mask, b_labels = batch  # 배치에서 입력 ID, 마스크, 라벨 추출\n",
    "\n",
    "        # 모델에 배치를 전달하여 손실값 계산\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "        # 손실값 추출\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()  # 기울기(gradient) 초기화\n",
    "        loss.backward()  # 역전파를 통해 기울기(gradient) 계산\n",
    "        optimizer.step()  # 매개변수 업데이트\n",
    "\n",
    "        total_train_loss += loss.item()  # 총 손실에 더함\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)  # 평균 학습 손실 계산\n",
    "\n",
    "    return avg_train_loss  # 평균 학습 손실 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "10c23dd8-ed93-4316-996b-5d8c86d81e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, validation_dataloader, device):\n",
    "    \"\"\"\n",
    "    모델을 사용하여 검증 데이터셋에 대한 평가를 수행하는 함수입니다.\n",
    "\n",
    "    Parameters:\n",
    "    model (torch.nn.Module): 평가할 모델 객체.\n",
    "    validation_dataloader (torch.utils.data.DataLoader): 검증 데이터셋의 DataLoader.\n",
    "    device (torch.device): 평가에 사용할 장치(CPU 또는 CUDA).\n",
    "\n",
    "    Returns:\n",
    "    float: 평균 검증 손실값.\n",
    "    dict: 다양한 평가 지표(metrics)에 대한 값들을 담은 사전.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "\n",
    "    total_eval_loss = 0  # 검증 손실을 누적할 변수 초기화\n",
    "    predictions, true_labels = [], []  # 예측값과 실제 라벨값을 저장할 리스트 초기화\n",
    "\n",
    "    # 검증 데이터로더를 순회하며 배치 단위로 평가\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)  # 배치 데이터를 디바이스로 이동\n",
    "        b_input_ids, b_input_mask, b_labels = batch  # 배치에서 입력 ID, 마스크, 라벨 추출\n",
    "\n",
    "        with torch.no_grad():  # 기울기(gradient) 계산을 수행하지 않음\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "        # 모델 출력에서 손실값 추출\n",
    "        if outputs.loss is not None:\n",
    "            loss = outputs.loss\n",
    "            total_eval_loss += loss.item()  # 총 손실에 더함\n",
    "\n",
    "        logits = outputs.logits.detach().cpu().numpy()  # 모델 예측값(로짓)을 numpy 배열로 변환\n",
    "        label_ids = b_labels.to('cpu').numpy()  # 실제 라벨값을 numpy 배열로 변환\n",
    "\n",
    "        # 3개의 값 중 가장 큰 값을 예측한 인덱스로 결정 (예시: logits = [3.513, -0.309, -2.111] ==> 예측: 0)\n",
    "        predictions.extend(np.argmax(logits, axis=1).flatten()) # 예측된 클래스를 리스트에 추가\n",
    "        true_labels.extend(label_ids.flatten()) # 실제 레이블 값을 리스트에 추가\n",
    "\n",
    "    eval_metrics = metrics(predictions, true_labels)\n",
    "\n",
    "    return total_eval_loss / len(validation_dataloader), eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bfa7c2ea-2208-432a-9990-09b3b8cda78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch: 97it [00:15,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.48\n",
      "  Accuracy: 0.80\n",
      "  F1 Macro: 0.77\n",
      "  F1 Micro: 0.80\n",
      "  F1 Weighted: 0.80\n",
      "Validation loss decreased (inf --> 0.48).  Saving model ...\n",
      "======== Epoch 2 / 3 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch: 97it [00:15,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.45\n",
      "  Accuracy: 0.82\n",
      "  F1 Macro: 0.78\n",
      "  F1 Micro: 0.82\n",
      "  F1 Weighted: 0.82\n",
      "Validation loss decreased (0.48 --> 0.45).  Saving model ...\n",
      "======== Epoch 3 / 3 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch: 97it [00:15,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.48\n",
      "  Accuracy: 0.85\n",
      "  F1 Macro: 0.81\n",
      "  F1 Micro: 0.85\n",
      "  F1 Weighted: 0.85\n"
     ]
    }
   ],
   "source": [
    "# 최소 검증 손실 초기화\n",
    "min_val_loss = float('inf')\n",
    "\n",
    "# 메인 학습 & 평가 루프\n",
    "for epoch_i in range(0, epochs):\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "\n",
    "    # 학습 단계\n",
    "    train_epoch(model, train_dataloader, optimizer, device)\n",
    "\n",
    "    print(\"\\nRunning Validation...\")\n",
    "    # 검증 단계\n",
    "    avg_val_loss, eval_metrics = evaluate(model, validation_dataloader, device)\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_metrics['accuracy']))\n",
    "    print(\"  F1 Macro: {0:.2f}\".format(eval_metrics['f1_macro']))\n",
    "    print(\"  F1 Micro: {0:.2f}\".format(eval_metrics['f1_micro']))\n",
    "    print(\"  F1 Weighted: {0:.2f}\".format(eval_metrics['f1_weighted']))\n",
    "\n",
    "    # 검증 손실이 현재까지의 최소값보다 작은 경우 체크포인트 저장\n",
    "    if avg_val_loss < min_val_loss:\n",
    "        print(f\"Validation loss decreased ({min_val_loss:.2f} --> {avg_val_loss:.2f}).  Saving model ...\")\n",
    "        # 베스트 모델 저장\n",
    "        torch.save(model.state_dict(), 'model_checkpoint.pt')\n",
    "        # 최소 검증 손실 업데이트\n",
    "        min_val_loss = avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e591548a-2a65-43d2-b398-cd403636affa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Test Loss: 0.43\n",
      "  Accuracy: 0.82\n",
      "  F1 Macro: 0.81\n",
      "  F1 Micro: 0.82\n",
      "  F1 Weighted: 0.82\n"
     ]
    }
   ],
   "source": [
    "# 베스트 모델 로드\n",
    "model.load_state_dict(torch.load(\"model_checkpoint.pt\"))\n",
    "\n",
    "avg_val_loss, eval_metrics = evaluate(model, test_dataloader, device)\n",
    "print(\"  Test Loss: {0:.2f}\".format(avg_val_loss))\n",
    "print(\"  Accuracy: {0:.2f}\".format(eval_metrics['accuracy']))\n",
    "print(\"  F1 Macro: {0:.2f}\".format(eval_metrics['f1_macro']))\n",
    "print(\"  F1 Micro: {0:.2f}\".format(eval_metrics['f1_micro']))\n",
    "print(\"  F1 Weighted: {0:.2f}\".format(eval_metrics['f1_weighted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "74d18b5d-ca0e-4709-adf7-4f75c90c09d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=model.cuda(), tokenizer=tokenizer, device=0, max_length=512, return_all_scores=True, function_to_apply='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4f538efe-88b5-48e2-98fb-2e41aeb743f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.04614420607686043}, {'label': 'LABEL_1', 'score': 0.9536319375038147}, {'label': 'LABEL_2', 'score': 0.00022387866920325905}]]\n"
     ]
    }
   ],
   "source": [
    "result = pipe('SK하이닉스가 매출이 급성장하였다')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "94caaaaf-d8c1-452e-947b-e6bd62cd68e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.9536319375038147}]\n"
     ]
    }
   ],
   "source": [
    "# return_all_scores 제거\n",
    "pipe = pipeline(\"text-classification\", model=model.cuda(), tokenizer=tokenizer, device=0, max_length=512, function_to_apply='softmax')\n",
    "\n",
    "result = pipe('SK하이닉스가 매출이 급성장하였다')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ed914cb7-a2d7-4395-9e33-abae0baeaf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'LABEL_0' : '중립', 'LABEL_1' : '긍정', 'LABEL_2' : '부정'}\n",
    "\n",
    "def prediction(text):\n",
    "  result = pipe(text)\n",
    "\n",
    "  return [label_dict[result[0]['label']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4a92a077-17d0-4c1d-a828-576cff095ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['긍정']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction('네이버가 매출이 급성장하였다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f36cf703-c0e5-4b59-b0c5-ce0c54a54424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['부정']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction('ChatGPT의 등장으로 인공지능 스타트업들은 위기에 빠졌다. 이에 OpenAI를 제외한 기타 스타트업들의 주가가 하락하였다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5ae19547-10c0-4abb-9f97-65ad8c33cf2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['중립']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction('인공지능 기술의 발전으로 누군가는 기회를 얻을 것이고, 누군가는 얻지 못할 것이다')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
